{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Step 0. Download data\n",
    "\n",
    "Data can be found at http://ltdata1.informatik.uni-hamburg.de/joint/hyperwatset/konvens/ \n",
    "\n",
    "Download all files and put them into the ``data`` directory to run the code below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1.Generate vectors for synsets and hyper synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aishwarya\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "2022-03-21 14:57:45,819 : INFO : loading projection weights from data/GoogleNews-vectors-negative300.txt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/GoogleNews-vectors-negative300.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\AISHWA~1\\AppData\\Local\\Temp/ipykernel_7932/3289529081.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mw2v_ru_original_fpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"data/all.norm-sz500-w10-cb0-it3-min5.w2v\"\u001b[0m \u001b[1;31m# RDT word2vec model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mw2v_en\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2v_en_fpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_and_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v_en_original_fpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[0mw2v_ru\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2v_ru_fpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_and_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v_ru_original_fpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\AISHWA~1\\AppData\\Local\\Temp/ipykernel_7932/3289529081.py\u001b[0m in \u001b[0;36mload_and_pickle\u001b[1;34m(w2v_fpath, binary)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mw2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v_pkl_fpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mw2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v_fpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mw2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mtry_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"for\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1631\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m         \"\"\"\n\u001b[1;32m-> 1633\u001b[1;33m         return _load_word2vec_format(\n\u001b[0m\u001b[0;32m   1634\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1635\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1894\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1895\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1896\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1897\u001b[0m             \u001b[1;31m# deduce both vocab_size & vector_size from 1st pass over file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m     fobj = _shortcut_open(\n\u001b[0m\u001b[0;32m    189\u001b[0m         \u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'errors'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 361\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/GoogleNews-vectors-negative300.txt'"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import logging\n",
    "from time import time\n",
    "from os.path import exists\n",
    "\n",
    "\n",
    "def try_print(w2v, test_word):\n",
    "    try:\n",
    "        for word, score in w2v.most_similar(test_word):\n",
    "            print(word, score)\n",
    "    except:\n",
    "        print(\"Warning: word '{}' not found.\".format(test_word))\n",
    "        \n",
    "    \n",
    "def load_and_pickle(w2v_fpath, binary=False):\n",
    "    tic = time()\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    w2v_pkl_fpath = w2v_fpath + \".pkl\"\n",
    "\n",
    "    if exists(w2v_pkl_fpath):\n",
    "        w2v = KeyedVectors.load(w2v_pkl_fpath)\n",
    "    else:\n",
    "        w2v = KeyedVectors.load_word2vec_format(w2v_fpath, binary=binary, unicode_errors='ignore')\n",
    "        w2v.init_sims(replace=True)\n",
    "        try_print(w2v, \"for\")\n",
    "        try_print(w2v, \"для\")\n",
    "        w2v.save(w2v_pkl_fpath)\n",
    "    \n",
    "    print(time()- tic, \"sec.\")\n",
    "\n",
    "    return w2v, w2v_pkl_fpath\n",
    "\n",
    "w2v_en_original_fpath = \"data/GoogleNews-vectors-negative300.txt\" # standard google news word2vec model\n",
    "w2v_ru_original_fpath = \"data/all.norm-sz500-w10-cb0-it3-min5.w2v\" # RDT word2vec model\n",
    "\n",
    "w2v_en, w2v_en_fpath = load_and_pickle(w2v_en_original_fpath)\n",
    "w2v_ru, w2v_ru_fpath = load_and_pickle(w2v_ru_original_fpath, binary=True)\n",
    "\n",
    "from glob import glob \n",
    "from vector_representations.build_sense_vectors import run\n",
    "\n",
    "for lang in [\"ru\", \"en\"]:\n",
    "    sensegram_fpaths = \"data/{}/*-sensegram.tsv\".format(lang)\n",
    "    w2v_fpath = w2v_ru_original_fpath if \"ru\" else w2v_en_original_fpath \n",
    "\n",
    "    for inventory_fpath in glob(sensegram_fpaths):\n",
    "        run(inventory_fpath, w2v_fpath) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Generate binary hypernyms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import operator\n",
    "from multiprocessing import Pool\n",
    "from vector_representations.dense_sense_vectors import DenseSenseVectors\n",
    "from traceback import format_exc\n",
    "from glob import glob \n",
    "\n",
    "\n",
    "def generate_binary_hypers(output_dir, max_synsets=1, hyper_synset_max_size=10, hc_max=0):\n",
    "    output_fpath = output_dir + \".vector-link-s%d-hmx%d-hc%d.csv\" % (\n",
    "        max_synsets, hyper_synset_max_size, hc_max)  \n",
    "    bin_count = 0\n",
    "    \n",
    "    out = codecs.open(output_fpath, \"w\", \"utf-8\")\n",
    "    log = codecs.open(output_fpath + \".log\", \"w\", \"utf-8\")\n",
    "    \n",
    "    for i, h_id in enumerate(dsv.pcz.data):\n",
    "        try:\n",
    "            if i % 10000 == 0: print(i)\n",
    "\n",
    "            if \"h\" in h_id:\n",
    "                hypo_h_senses = dsv.pcz.data[h_id][0][\"cluster\"]\n",
    "                tmp = sorted(dsv.pcz.data[h_id][0][\"cluster\"].items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "                s_id = \"s\" + h_id[1:]\n",
    "                hypo_senses = dsv.pcz.data[s_id][0][\"cluster\"]\n",
    "                log.write(\"\\n{}\\t{}\\n\".format(\n",
    "                    h_id, \", \".join(hypo_h_senses)\n",
    "                ))\n",
    "                log.write(\"{}\\n\".format(\n",
    "                    \", \".join([\"{}:{}\".format(k,v) for k,v in tmp])\n",
    "                ))\n",
    "                log.write(\"{}\\t{}\\n\".format(\n",
    "                    s_id, \", \".join(hypo_senses)\n",
    "                ))\n",
    "\n",
    "                # save relations from the hierarchical context \n",
    "                for hypo_sense in hypo_senses:\n",
    "                    for hc_num, hyper_sense in enumerate(hypo_h_senses):\n",
    "                        if hc_num == hc_max: break\n",
    "                        hypo_word = hypo_sense.split(\"#\")[0]\n",
    "                        hyper_word = hyper_sense.split(\"#\")[0]\n",
    "                        if hypo_word != hyper_word:\n",
    "                            out.write(\"{}\\t{}\\tfrom-original-labels\\n\".format(hypo_word, hyper_word))\n",
    "                    bin_count += 1\n",
    "\n",
    "                # save binary relations from a synset\n",
    "                s_synsets = 0\n",
    "                for rh_id, s in dsv.sense_vectors.most_similar(h_id + \"#0\"):\n",
    "                    if \"s\" in rh_id:\n",
    "                        hyper_senses = dsv.pcz.data[rh_id.split(\"#\")[0]][0][\"cluster\"]\n",
    "                        if len(hyper_senses) > hyper_synset_max_size: continue\n",
    "\n",
    "                        rh_str = \", \".join(hyper_senses)\n",
    "                        log.write(\"\\t{}:{:.3f} {}\\n\".format(rh_id, s, rh_str))\n",
    "\n",
    "                        for hypo_sense in hypo_senses:\n",
    "                            for hyper_sense in hyper_senses:\n",
    "                                hypo_word = hypo_sense.split(\"#\")[0]\n",
    "                                hyper_word = hyper_sense.split(\"#\")[0]\n",
    "                                if hypo_word != hyper_word:\n",
    "                                    out.write(\"{}\\t{}\\tfrom-vector-linkage\\n\".format(hypo_word, hyper_word))\n",
    "                                bin_count += 1\n",
    "                        s_synsets += 1\n",
    "\n",
    "                        if s_synsets >= max_synsets: break\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "        except:\n",
    "            print(\"Error\", i, h_id)\n",
    "            print(format_exc())\n",
    "    out.close()\n",
    "    log.close()\n",
    "    \n",
    "    print(\"# binary relations:\", bin_count)\n",
    "    print(\"binary relations:\", output_fpath)\n",
    "    print(\"log of binary relations:\", output_fpath + \".log\")\n",
    "    \n",
    "    return bin_count, output_fpath\n",
    "    \n",
    "\n",
    "for pcz_fpath in glob(\"data/ru/*tsv\"):\n",
    "    print(pcz_fpath)\n",
    "    reload = True\n",
    "    try: dsv\n",
    "    except NameError: reload = True\n",
    "\n",
    "    if reload:\n",
    "        dsv = DenseSenseVectors(\n",
    "            pcz_fpath=pcz_fpath,\n",
    "            word_vectors_obj=None,\n",
    "            save_pkl=True,\n",
    "            sense_dim_num=1000,\n",
    "            norm_type=\"sum\",\n",
    "            weight_type=\"score\",\n",
    "            max_cluster_words=20)\n",
    "\n",
    "    for max_top_synsets in [1, 2, 3]:\n",
    "        for max_hyper_synset_size in [3, 5, 10, 20]:\n",
    "            for hc_max in [1, 3, 5]: \n",
    "                print(\"=\"*50)\n",
    "                print(\"max number of synsets:\", max_top_synsets)\n",
    "                print(\"max hyper synset size:\", max_hyper_synset_size)\n",
    "                print(\"hc_max:\", hc_max)\n",
    "                generate_binary_hypers(pcz_fpath, max_top_synsets, max_hyper_synset_size, hc_max)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
